{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install konlpy\n",
    "# !pip install pandas\n",
    "# !pip install scikit-learn\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model, save_model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dropout, Bidirectional, Attention, Concatenate, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"png2x\") # svg, retina, png2x ...\n",
    "mpl.style.use(\"seaborn-v0_8\")\n",
    "mpl.rcParams.update({\"figure.constrained_layout.use\": True})\n",
    "sns.set_context(\"paper\") \n",
    "sns.set_palette(\"Set2\") \n",
    "sns.set_style(\"whitegrid\") \n",
    "\n",
    "plt.rc(\"font\", family = \"Malgun Gothic\")\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# 파일 저장시 파일명의 용이성\n",
    "def now_time():\n",
    "    now = datetime.now()\n",
    "    return now.strftime('%Y%m%d_%H_%M_%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir_1 = 'data_prep/data/_2_after_prep/corpus_method_1/'\n",
    "base_dir_2 = 'data_prep/data/_2_after_prep/corpus_method_2/'\n",
    "\n",
    "\n",
    "with open(base_dir_1 + \"sentences.pkl\",\"rb\") as f:\n",
    "    sentences = pickle.load(f)\n",
    "\n",
    "with open(base_dir_2 + \"labels.pkl\",\"rb\") as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "with open(base_dir_1 + \"sentences_corpus_word_index.json\",\"r\") as f:\n",
    "    sentences_corpus_word_index = json.load(f)\n",
    "\n",
    "with open(base_dir_2 + \"label_corpus_word_index.json\",\"r\") as f:\n",
    "    label_corpus_word_index = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 패딩 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이렇게 분리 하는 이유 (기존의 트레인 테스트의 tsv를 하나로 합쳤기 때문)\n",
    "train_sentences, test_sentences = sentences[:15005], sentences[15005:] \n",
    "train_labels, test_labels = labels[:15005], labels[15005:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_data = deepcopy(train_labels) # 디코더의 입력 : end 토큰만 제외\n",
    "target_sequences   = deepcopy(train_labels)   # 타겟 데이터 : 에서 start 토큰을 제외한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 라벨에 각 요소별 시작 1 토큰 추가 및 끝토큰 추가\n",
    "for i in decoder_input_data:\n",
    "    i.insert(0,1)\n",
    "for i in target_sequences:\n",
    "    i.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3]\n",
      "[3, 2]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input_data[0])\n",
    "print(target_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 코퍼스 인덱스 : 단어 역변환 (나중에 찾기 쉽게하기 위함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_corpus_index_word = {sentences_corpus_word_index[key]:key for key in sentences_corpus_word_index}\n",
    "label_corpus_index_word = {label_corpus_word_index[key]:key for key in label_corpus_word_index}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 패딩 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_text_len = max([len(i) for i in train_sentences])\n",
    "text_padded = pad_sequences(train_sentences, maxlen=max_text_len, padding='post')\n",
    "\n",
    "max_label_len = max([len(i) for i in decoder_input_data])\n",
    "decoder_input_data_padded = pad_sequences(decoder_input_data, maxlen=max_label_len, padding='post')\n",
    "target_sequences_padded = pad_sequences(target_sequences, maxlen=max_label_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전체 코퍼스 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38712, 19)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_size = len(sentences_corpus_word_index)\n",
    "corpus_size_label = len(label_corpus_word_index)\n",
    "corpus_size, corpus_size_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "멸치 가 틀 딱 짜증나서 트러블 나면 조직 적 으로 좀 스럽게 보복 함 ex : 수건 찜 유도탄 , 틀 한 명 한 명의 살인 광선 , 긴박할 때 말 걸어서 사고 유도 , 좁은 곳 에서 안 비키기 , 샤워 할 때 후 장 씻으며 물 튀기기 \n",
      "연령\n",
      "참군 남이노 남자 의 용도 는 고 기 방패 다 ㅋㅋㅋ \n",
      "남성\n"
     ]
    }
   ],
   "source": [
    "# 불러온 데이터 확인하기\n",
    "for i in train_sentences[-1]:\n",
    "    print(sentences_corpus_index_word[i], end=' ')\n",
    "\n",
    "print(\"\")\n",
    "for i in train_labels[-1]:\n",
    "    print(label_corpus_index_word[i])\n",
    "\n",
    "# 불러온 데이터 확인하기\n",
    "for i in test_sentences[-1]:\n",
    "    print(sentences_corpus_index_word[i], end=' ')\n",
    "\n",
    "print(\"\")\n",
    "for i in test_labels[-1]:\n",
    "    print(label_corpus_index_word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 19)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_label_len, corpus_size_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model._1_lstm_with_attention_model_2_cleaned as model_lstm_att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# best 파라미터로 훈련시켜 최종 모델 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 38712, 10, 19)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_text_len, corpus_size, max_label_len, corpus_size_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 77)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 77, 2048)     79282176    ['input_13[0][0]']               \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)          [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 77, 2048)     0           ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        multiple             38912       ['input_14[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  [(None, 77, 2048),  25174016    ['dropout_8[0][0]']              \n",
      " )                               (None, 1024),                                                    \n",
      "                                 (None, 1024),                                                    \n",
      "                                 (None, 1024),                                                    \n",
      "                                 (None, 1024)]                                                    \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 10, 2048)     0           ['embedding_5[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 2048)         0           ['bidirectional_2[0][1]',        \n",
      "                                                                  'bidirectional_2[0][3]']        \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 2048)         0           ['bidirectional_2[0][2]',        \n",
      "                                                                  'bidirectional_2[0][4]']        \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)                  multiple             33562624    ['dropout_10[0][0]',             \n",
      "                                                                  'concatenate_8[0][0]',          \n",
      "                                                                  'concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 10, 2048)     0           ['lstm_5[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 77, 2048)     0           ['bidirectional_2[0][0]']        \n",
      "                                                                                                  \n",
      " attention_2 (Attention)        multiple             0           ['dropout_11[0][0]',             \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 10, 4096)     0           ['dropout_11[0][0]',             \n",
      "                                                                  'attention_2[0][0]']            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                multiple             77843       ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 138,135,571\n",
      "Trainable params: 138,135,571\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1000\n",
      "106/106 [==============================] - 31s 204ms/step - loss: 0.2344 - accuracy: 0.7287 - val_loss: 0.1384 - val_accuracy: 0.8216\n",
      "Epoch 2/1000\n",
      "106/106 [==============================] - 18s 169ms/step - loss: 0.1032 - accuracy: 0.8787 - val_loss: 0.1299 - val_accuracy: 0.8384\n",
      "Epoch 3/1000\n",
      "106/106 [==============================] - 18s 166ms/step - loss: 0.0560 - accuracy: 0.9359 - val_loss: 0.1481 - val_accuracy: 0.8407\n"
     ]
    }
   ],
   "source": [
    "# 실험 1 및 결과\n",
    "# Embedding1s = [32,64,128,256,512,1024]\n",
    "# Embedding2s = [32,64,128,256,512,1024]\n",
    "# lstm_sizes = [32,64,128,256,512,1024]\n",
    "# dropout_ratios = [0.2,0.4,0.6,0.8]\n",
    "\n",
    "# 해석 결과 : 임베딩 사이즈를 조금 더 늘려서 실험해보는 작업을 해야 겠다.\n",
    "#############################################################\n",
    "\n",
    "# 실험 2 및 결과\n",
    "# Embedding1s = [2048]\n",
    "# Embedding2s = [2048]\n",
    "# lstm_sizes = [64,256,1024,2048]\n",
    "# dropout_ratios = [0.2,0.4,0.6,0.8] - 컴퓨터의 한계로인해 gpu 다운\n",
    "\n",
    "# 실험 2 이어서 - 이후 코랩 진행 결과\n",
    "# Embedding1s = [2048]\n",
    "# Embedding2s = [2048]\n",
    "# lstm_sizes = [2048]\n",
    "# dropout_ratios = [0.2,0.4,0.6,0.8]\n",
    "\n",
    "# Best\n",
    "# embedding_size1=2048, embedding_size2=2048, lstm_size=1024, dropout_ratio=0.6 의 결과 최소 loss 값: 0.1358497142791748\n",
    "#############################################################\n",
    "\n",
    "# 추가실험 멀티 LSTM 층 추가\n",
    "# 인코더 디코더에 각 각 총 2층의 LSTM\n",
    "# 총 학습 파라미터\n",
    "# Total params: 255,608,851\n",
    "# 더 나은 결과를 얻지 못함\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# 추가실험 멀티 LSTM 층 추가\n",
    "# 인코더에 총 2 층의 LSTM과 디코더에는 단 1층의 LSTM\n",
    "# 총 학습 파라미터\n",
    "# Total params: 166,~~~,~~~\n",
    "# 더나은 결과를 얻지못함\n",
    "\n",
    "############################################################\n",
    "#\n",
    "# 이후 각 인코더 및 디코더에 dense 층을 늘려보고 정규화 과정을 해보았지만,\n",
    "# 큰 개선은 없어보임\n",
    "#\n",
    "#############################################################\n",
    "# 결국 각 1층씩의 LSTM\n",
    "\n",
    "# 최종 파라미터\n",
    "# Total params: 138,135,571\n",
    "\n",
    "# embedding1 = 2048\n",
    "# embedding2 = 2048\n",
    "# lstm_size = 1024\n",
    "# dropout_ratio = 0.6\n",
    "##############################################################\n",
    "\n",
    "embedding1 = 2048\n",
    "embedding2 = 2048\n",
    "lstm_size = 1024\n",
    "dropout_ratio = 0.6\n",
    "\n",
    "model, encoder_model, decoder_model = \\\n",
    "model_lstm_att.seq2seq_with_attention(\n",
    "    max_text_len,\n",
    "    corpus_size,\n",
    "    max_label_len,\n",
    "    corpus_size_label,\n",
    "    embedding_size1=embedding1,\n",
    "    embedding_size2=embedding2,\n",
    "    lstm_size=lstm_size,\n",
    "    dropout_ratio=dropout_ratio)\n",
    "\n",
    "model.summary()\n",
    "# # 모델 학습\n",
    "check_path = 'checkpoint'\n",
    "check_path_list = os.listdir(check_path)\n",
    "# # 학습시\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=1, min_delta=0.001)\n",
    "checkpoint = ModelCheckpoint(check_path + f'/epoch_'+'{epoch:04d}_metrics_{loss:.4f},{accuracy:.4f},{val_loss:.4f},{val_accuracy:.4f}.h5', save_freq='epoch')\n",
    "# 모델 학습\n",
    "# 입력 데이터 : 패딩된 텍스트 시퀀스\n",
    "# 디코더 인풋데이터 : 끝 토큰을 제외한 값\n",
    "# 타겟 데이터 : 시작 토큰을 제외한 값\n",
    "\n",
    "history_model = model.fit([text_padded, decoder_input_data_padded], target_sequences_padded,\n",
    "                        validation_split=0.1,\n",
    "                        epochs=1000,\n",
    "                        initial_epoch=len(check_path_list),\n",
    "                        batch_size=128,\n",
    "                        callbacks=[early_stopping,checkpoint],\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_model('final_model.h5', compile=False)\n",
    "model.set_weights(_.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_for_predict = sentences_corpus_word_index,max_text_len, label_corpus_word_index, label_corpus_index_word, max_label_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clean']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moonjang_ = '안녕 하십니까 반가워요? '\n",
    "model_lstm_att.predict_from_seq(moonjang_, encoder_model, decoder_model, sets_for_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 데이터를 이용하여 정답 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 정확도: 63.25929890286326\n",
      "하나라도 겹치는것이 있는 비율: 69.44072785656944\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "has_more_than_one = 0\n",
    "sum_pred = 0\n",
    "predict_labels = []\n",
    "correct_labels = []\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sen = ' '.join([sentences_corpus_index_word[j] for j in test_sentences[i]])\n",
    "    predict_label = model_lstm_att.predict_from_seq(test_sen, encoder_model, decoder_model, sets_for_predict)\n",
    "    correct_label = [label_corpus_index_word[k] for k in test_labels[i]]\n",
    "\n",
    "    # 정확도 계산\n",
    "    sum_pred += 1\n",
    "    correct += 1 if sorted(predict_label) == sorted(correct_label) else 0\n",
    "\n",
    "    # 부분 일치 처리\n",
    "    has_more_than_one += 1 if set(predict_label).intersection(set(correct_label)) else 0\n",
    "\n",
    "    predict_labels.append(predict_label)\n",
    "    correct_labels.append(correct_label)\n",
    "\n",
    "    # 진행 상황 출력\n",
    "    clear_output(wait=True)\n",
    "    print(f'현재 인덱스 진행 ({i + 1}/{len(test_sentences)})')\n",
    "    print('정답률:', correct / sum_pred * 100)\n",
    "    print('하나라도 겹치는것이 있는 비율:', has_more_than_one / sum_pred * 100)\n",
    "\n",
    "    print(f'''\n",
    "    문제 : {test_sen}\n",
    "    정답 : {correct_label} | 예측 : {predict_label}\n",
    "    {('정답!' if predict_label == correct_label else '오답!')}\n",
    "    ''')\n",
    "clear_output(wait=True)\n",
    "print('최종 정확도:', correct / sum_pred * 100)\n",
    "print('하나라도 겹치는것이 있는 비율:', has_more_than_one / sum_pred * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 고찰\n",
    "학습시 train 데이터에 대한 정확도에 비해 현저히 낮은 정확도의 현상을 보이고있다. <br>\n",
    "교사 학습으로 인해 학습시에는 좀더 높은 정확도가 나왔을 것이라 판단 된다. <br>\n",
    "\n",
    "그럼에도 불구하고, 단순 인코더와 dense 층의 모델과 큰 정확도 차이를 보여준다. 아래 코드와 정확도 비교<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2dense 과의 비교\n",
    "- valid 데이터에서의 결과 50퍼센트를 넘기지 못함...\n",
    "```py\n",
    "for i in np.arange(0.2,0.31,0.001):\n",
    "    pred = model.predict(X_test,verbose=0) > 0.2\n",
    "\n",
    "    number_of_count = 0\n",
    "    correct = 0\n",
    "    for j in pred == y_test.to_numpy():\n",
    "        if False not in j:\n",
    "            correct +=1\n",
    "        number_of_count +=1\n",
    "    print(f'트레쉬 홀드가 {i}일때 정확도',correct/number_of_count)\n",
    "\n",
    "# incoder 와 Dense 층만 있는 모델의 valid 데이터에 대한 정확도\n",
    "# 트레쉬 홀드가 0.00일때 정확도 0.00\n",
    "# 트레쉬 홀드가 0.10일때 정확도 0.40\n",
    "# 트레쉬 홀드가 0.20일때 정확도 0.45\n",
    "# 트레쉬 홀드가 0.30일때 정확도 0.46\n",
    "# 트레쉬 홀드가 0.40일때 정확도 0.46\n",
    "# 트레쉬 홀드가 0.50일때 정확도 0.44\n",
    "# 트레쉬 홀드가 0.60일때 정확도 0.43\n",
    "# 트레쉬 홀드가 0.70일때 정확도 0.40\n",
    "# 트레쉬 홀드가 0.80일때 정확도 0.37\n",
    "# 트레쉬 홀드가 0.90일때 정확도 0.31\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 틀린 문장 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 테스트 데이터 number 까지의\n",
    "# number = 10\n",
    "# print(\"완전 정확히 못맞춘 문장들\")\n",
    "# for s,c,p in zip(test_sentences[:number],correct_labels[:number],predict_labels[:number]):\n",
    "#     if c != p:\n",
    "#         sen = ''\n",
    "#         for w in s:\n",
    "#             sen += sentences_corpus_index_word[w] + ' '\n",
    "#         print(\"문장: \",sen)\n",
    "#         print(\"정답: \",c,\",예측: \",p)\n",
    "#         print(\" \")\n",
    "        \n",
    "# print(\"-\"*100)\n",
    "# print(\"하나도 못 맞춘 문장들\")\n",
    "# for s,c,p in zip(test_sentences[:number],correct_labels[:number],predict_labels[:number]):\n",
    "#     if c != p:\n",
    "#         sen = ''\n",
    "#         for w in s:\n",
    "#             sen += sentences_corpus_index_word[w] + ' '\n",
    "#         if set(c).intersection(set(p)) == set():\n",
    "#             print(\"문장: \",sen)\n",
    "#             print(\"정답: \",c,\",예측: \",p)\n",
    "#             print(\" \")\n",
    "\n",
    "# print(\"-\"*100)\n",
    "# print(\"하나라도 맞춘 문장들\")\n",
    "# for s,c,p in zip(test_sentences[:number],correct_labels[:number],predict_labels[:number]):\n",
    "#     if c != p:\n",
    "#         sen = ''\n",
    "#         for w in s:\n",
    "#             sen += sentences_corpus_index_word[w] + ' '\n",
    "#         if set(c).intersection(set(p)) != set():\n",
    "#             print(\"문장: \",sen)\n",
    "#             print(\"정답: \",c,\",예측: \",p)\n",
    "#             print(\" \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
