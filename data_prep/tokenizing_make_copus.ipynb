{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dropout, Bidirectional, Attention, Concatenate, Dense\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import trange\n",
    "\n",
    "from copy import deepcopy\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# 그래프를 위한 라이브러리 및 초기 그래프 설정\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"png2x\") # svg, retina, png2x ...\n",
    "mpl.style.use(\"seaborn-v0_8\")\n",
    "mpl.rcParams.update({\"figure.constrained_layout.use\": True})\n",
    "sns.set_context(\"paper\") \n",
    "sns.set_palette(\"Set2\") \n",
    "sns.set_style(\"whitegrid\") \n",
    "\n",
    "plt.rc(\"font\", family = \"Malgun Gothic\")\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# 파일 저장시 파일명의 용이성\n",
    "def now_time():\n",
    "    now = datetime.now()\n",
    "    return now.strftime('%Y%m%d_%H_%M_%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/_1_before_prep/unsmile_train_v1.0.tsv', sep='\\t')\n",
    "df_valid = pd.read_csv('data/_1_before_prep/unsmile_valid_v1.0.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train,df_valid],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('data/1_before_prep/all_data_unsmile.csv')\n",
    "df = pd.read_csv('data/_1_before_prep/all_data_unsmile.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [[i for i in j if i != ''] for j in (df.values[:,1:] * df.columns[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=18742, step=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.index #순차적인 인덱스가 필요 (for문을 위해)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라벨 코퍼스 및 라벨 수치화 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## corpus_method_1 를 위한 정답 라벨\n",
    "- input 문장 : Okp()의 morphs 이용\n",
    "- output 문장 : 각 카테고리의 묶음을 정답 라벨의 corpus 로 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'padding': 0,\n",
       "  'start': 1,\n",
       "  'end': 2,\n",
       "  'clean': 3,\n",
       "  '종교': 4,\n",
       "  '여성/가족': 5,\n",
       "  '인종/국적': 6,\n",
       "  '지역': 7,\n",
       "  '기타 혐오': 8,\n",
       "  '악플/욕설': 9,\n",
       "  '성소수자': 10,\n",
       "  '개인지칭': 11,\n",
       "  '남성': 12,\n",
       "  '연령': 13},\n",
       " [[3], [4], [3], [3], [5], [3], [6, 7, 4, 8], [9], [5], [3]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_corpus_word_index = {'padding':0,'start':1,'end':2}\n",
    "label_index=3\n",
    "for i in label:\n",
    "    for j in i:\n",
    "        if j not in label_corpus_word_index:\n",
    "            label_corpus_word_index[j] = label_index\n",
    "            label_index +=1\n",
    "\n",
    "\n",
    "labels = []\n",
    "for i in label:\n",
    "    single_label=[]\n",
    "    for j in (i):\n",
    "        single_label.append(label_corpus_word_index[j])\n",
    "    labels.append(single_label)\n",
    "\n",
    "\n",
    "label_corpus_word_index , labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 인덱스화 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18742/18742 [01:45<00:00, 178.04it/s]\n",
      "100%|██████████| 38712/38712 [00:00<00:00, 1858842.55it/s]\n",
      "100%|██████████| 18742/18742 [01:39<00:00, 187.75it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Okt()\n",
    "\n",
    "corpus=['padding']\n",
    "for i in trange(len(df)):\n",
    "    for j in tokenizer.morphs(df.iloc[i,0]):\n",
    "        if j not in corpus:\n",
    "            corpus.append(j)\n",
    "\n",
    "# 코퍼스 워드투 인덱스 과정\n",
    "sentences_corpus_word_index = {corpus[i]: i for i in trange(0,len(corpus))}\n",
    "\n",
    "sentences = []\n",
    "for i in trange(len(df)):\n",
    "    a_sentence = [sentences_corpus_word_index[i] for i in tokenizer.morphs(df.iloc[i,0])]\n",
    "    sentences.append(a_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 명사와 동사 등 기준으로 인덱스화 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모프 단위가 아닌 방법으로 해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장하기 목록\n",
    "1. 문장 데이터(시퀀스 데이터) : sentences.pkl\n",
    "2. 정답 데이터(시퀀스 화 된 데이터) : summarys.pkl\n",
    "3. 문장에 대한 코퍼스 : corpus_word_index.json\n",
    "4. 정답에 대한 코퍼스 : corpus_word_index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data\\_2_after_prep\\corpus_method_1\\sentences.pkl\",\"wb\") as f:\n",
    "    pickle.dump(sentences, f)\n",
    "    \n",
    "with open(\"data\\_2_after_prep\\corpus_method_1\\labels.pkl\",\"wb\") as f:\n",
    "    pickle.dump(labels, f)\n",
    "\n",
    "with open(\"data\\_2_after_prep\\corpus_method_1\\sentences_corpus_word_index.json\",\"w\") as f:\n",
    "    json.dump(sentences_corpus_word_index, f)\n",
    "\n",
    "with open(\"data\\_2_after_prep\\corpus_method_1/label_corpus_word_index.json\",\"w\") as f:\n",
    "    json.dump(label_corpus_word_index, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corpus_method_2 를 위한 정답 라벨\n",
    "- 정답 라벨을 단어 단위로 분리 시켜 seq2seq 학습에 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'padding': 0,\n",
       " 'start': 1,\n",
       " 'end': 2,\n",
       " 'clean': 3,\n",
       " '종교': 4,\n",
       " '여성': 5,\n",
       " '가족': 6,\n",
       " '인종': 7,\n",
       " '국적': 8,\n",
       " '지역': 9,\n",
       " '기타': 10,\n",
       " '혐오': 11,\n",
       " '악플': 12,\n",
       " '욕설': 13,\n",
       " '성소수자': 14,\n",
       " '개인': 15,\n",
       " '지칭': 16,\n",
       " '남성': 17,\n",
       " '연령': 18}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_corpus_method_2 = []\n",
    "for i in label:\n",
    "    for j in i:\n",
    "        for k in tokenizer.morphs(j):\n",
    "            if (k not in label_corpus_method_2) and (k != '/'): \n",
    "                label_corpus_method_2.append(k)\n",
    "\n",
    "label_corpus_word_index_method_2 = {'padding':0,'start':1,'end':2}\n",
    "label_index=3\n",
    "for l in label_corpus_method_2:\n",
    "    label_corpus_word_index_method_2[l] = label_index\n",
    "    label_index +=1\n",
    "\n",
    "label_corpus_word_index_method_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tokened = []\n",
    "for words in label:\n",
    "    single_label = []\n",
    "    for word in words:\n",
    "        for token in tokenizer.morphs(word):\n",
    "            if token != '/':\n",
    "                single_label.append(label_corpus_word_index_method_2[token])\n",
    "    labels_tokened.append(single_label)\n",
    "    # single_label.append(label_corpus_word_index_method_2[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] ['clean']\n",
      "[4] ['종교']\n",
      "[3] ['clean']\n",
      "[3] ['clean']\n",
      "[5, 6] ['여성/가족']\n",
      "[3] ['clean']\n",
      "[7, 8, 9, 4, 10, 11] ['인종/국적', '지역', '종교', '기타 혐오']\n",
      "[12, 13] ['악플/욕설']\n",
      "[5, 6] ['여성/가족']\n",
      "[3] ['clean']\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 잘 되었는지 확인\n",
    "for i,j in zip(labels_tokened[:10] , label[:10]):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 인덱스 토큰과, corpus 저장하기\n",
    "with open(\"data/_2_after_prep/corpus_method_2/labels.pkl\",\"wb\") as f:\n",
    "    pickle.dump(labels_tokened, f)\n",
    "\n",
    "with open(\"data/_2_after_prep/corpus_method_2/label_corpus_word_index.json\",\"w\") as f:\n",
    "    json.dump(label_corpus_word_index_method_2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  불러오기 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/_2_after_prep/corpus_method_1/sentences.pkl\",\"rb\") as f:\n",
    "    sentences = pickle.load(f)\n",
    "\n",
    "with open(\"data/_2_after_prep/corpus_method_1/labels.pkl\",\"rb\") as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "with open(\"data/_2_after_prep/corpus_method_2/labels.pkl\",\"rb\") as f:\n",
    "    labels2 = pickle.load(f)\n",
    "\n",
    "with open(\"data/_2_after_prep/corpus_method_1/sentences_corpus_word_index.json\",\"r\") as f:\n",
    "    sentences_corpus_word_index = json.load(f)\n",
    "\n",
    "with open(\"data/_2_after_prep/corpus_method_1/label_corpus_word_index.json\",\"r\") as f:\n",
    "    label_corpus_word_index = json.load(f)\n",
    "\n",
    "with open(\"data/_2_after_prep/corpus_method_2/label_corpus_word_index.json\",\"r\") as f:\n",
    "    label_corpus_word_index2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_corpus_index_word1 = {label_corpus_word_index[key]:key for key in label_corpus_word_index}\n",
    "label_corpus_index_word2 = {label_corpus_word_index2[key]:key for key in label_corpus_word_index2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seek_(number):   \n",
    "    print(number,'번째 인덱싱') \n",
    "    print(\"단어 인덱싱 :\",sentences[number])\n",
    "    print(\"문장의 원본 :\",df['문장'][number],end=\"\\n\\n\")\n",
    "    print('-'*100)\n",
    "    print(\"정답 라벨  :\" ,df.columns[1:].to_list())\n",
    "    print('라벨 방법 1:', labels[number] ,df.values[number,1:].tolist())\n",
    "    print('라벨 방법 2:', labels[number] ,',14 개의 코퍼스 인덱스 매칭:',[label_corpus_index_word1[i] for i in labels[number]])\n",
    "    print('라벨 방법 3:', labels2[number],',19 개의 코퍼스 인덱스 매칭:',[label_corpus_index_word2[i] for i in labels2[number]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "코퍼스 1   : {'padding': 0, 'start': 1, 'end': 2, 'clean': 3, '종교': 4, '여성/가족': 5, '인종/국적': 6, '지역': 7, '기타 혐오': 8, '악플/욕설': 9, '성소수자': 10, '개인지칭': 11, '남성': 12, '연령': 13}\n",
      "코퍼스 2   : {'padding': 0, 'start': 1, 'end': 2, 'clean': 3, '종교': 4, '여성': 5, '가족': 6, '인종': 7, '국적': 8, '지역': 9, '기타': 10, '혐오': 11, '악플': 12, '욕설': 13, '성소수자': 14, '개인': 15, '지칭': 16, '남성': 17, '연령': 18}\n"
     ]
    }
   ],
   "source": [
    "print('코퍼스 1   :', label_corpus_word_index)\n",
    "print('코퍼스 2   :', label_corpus_word_index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000 번째 인덱싱\n",
      "단어 인덱싱 : [77, 37822, 813, 37823]\n",
      "문장의 원본 : 여기 해피하우스아니냐\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "정답 라벨  : ['여성/가족', '남성', '성소수자', '인종/국적', '연령', '지역', '종교', '기타 혐오', '악플/욕설', 'clean', '개인지칭']\n",
      "라벨 방법 1: [3] [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "라벨 방법 2: [3] ,14 개의 코퍼스 인덱스 매칭: ['clean']\n",
      "라벨 방법 3: [3] ,19 개의 코퍼스 인덱스 매칭: ['clean']\n"
     ]
    }
   ],
   "source": [
    "i = 18000 #+=1\n",
    "seek_(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
